{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_seq2seq_tutorial_student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtyangpsp/HarvardEPS_MLclass2019/blob/master/lstm_seq2seq_tutorial_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDifyxoTX9LT",
        "colab_type": "text"
      },
      "source": [
        "<h1> Building a LSTM Encoder-Decoder using PyTorch to make Sequence-to-Sequence Predictions<h1> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBGyeF4fVWPd",
        "colab_type": "text"
      },
      "source": [
        "**Laura Kulowski & Cedric Flamant**<br>\n",
        "**Fall 2019**<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHF69Z6MV611",
        "colab_type": "text"
      },
      "source": [
        "<h2> Overview <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM1HlcCY60fZ",
        "colab_type": "text"
      },
      "source": [
        "There are many instances where we want to predict how a time series will behave in the future. For example, given the behavior of the stock market over the last month, we may want to know how the stock market will behave in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgXXwkHE8l8L",
        "colab_type": "text"
      },
      "source": [
        "<center> <img src=\"https://i.imgur.com/7uqw73X.png\" alt=\"\" width=\"500\"> </center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8Wpf4uF9KL",
        "colab_type": "text"
      },
      "source": [
        "Other examples of time series that we may wish to predict future values of include weather conditions (temperature, humidity, etc.), power usage, and traffic volume. The Long Short-Term Memory (LSTM) neural network is well-suited to these problems since the data may have long-term dependencies (i.e., past values may influence future values). \n",
        "\n",
        "Our goal in this lab is to make sequence-to-sequence predictions, or predictions where the input and output sequences might be different lengths, using a LSTM. For the stock market, this might involve providing the LSTM with 20 days of stock prices and predicting the next 5 days. To make sequence-to-sequence predictions, we use an LSTM with a special architecture: the LSTM encoder-decoder. \n",
        "\n",
        "The LSTM encoder-decoder involves combining two LSTMs. The first LSTM, or the encoder, processes the input sequence and outputs an encoded state (i.e., a summary of the input sequence). The second LSTM, or the decoder, uses the encoded state to generate an output sequence. The LSTM encoder-decoder architecture is shown below. \n",
        "\n",
        "<center> <img src=\"https://i.imgur.com/05OKp8k.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "In this lab, we will \n",
        "\n",
        "1.   Prepare a time series dataset to input into our LSTM encoder-decoder \n",
        "2.   Build the LSTM encoder-decoder using PyTorch\n",
        "3.   Train the model and make predictions \n",
        "4.   Evaulate our model on the train and test data\n",
        "5.   Experiment with the model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQKAw01PcAWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os, errno\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from tqdm import tnrange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nElP3T_IciRw",
        "colab_type": "text"
      },
      "source": [
        "<h2> 1. Preparing the Time Series Dataset <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrCS_Xb5bj0K",
        "colab_type": "text"
      },
      "source": [
        "<h3> Creating a Dataset <h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05X3bH2cefN_",
        "colab_type": "text"
      },
      "source": [
        "To make training easier, we will use a synthetic time series dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY2SPMf1dS8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create synthetic time series dataset\n",
        "def synthetic_data(Nt = 2000, tf = 80 * np.pi): \n",
        "  t = np.linspace(0., tf, Nt)\n",
        "  y = np.sin(2. * t) + 0.5 * np.cos(t) + np.random.normal(0., 0.2, Nt)\n",
        "  return t, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV_z4mYxdsxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t, y = synthetic_data() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m_STQWfl9g",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our time series data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQxE13X2d1s4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot time series \n",
        "## your code hre "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPp4Y_79bp-4",
        "colab_type": "text"
      },
      "source": [
        "<h3> Train/Test Split <h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQJ8pu5kc-lg",
        "colab_type": "text"
      },
      "source": [
        "Now, we will split our time series into train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adke2Wdgd2lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split time series into train/test sets \n",
        "def train_test_split(t, y, split = 0.8):\n",
        "  '''\n",
        "  : param t: time data \n",
        "  : para y: feature \n",
        "  : para split: percent of data to include in training set\n",
        "  : return: t/y training/test arrays (shape: [# samples, 1]) \n",
        "  '''\n",
        "  \n",
        "  indx_split = int(split * len(y))\n",
        "  indx_train = np.arange(0, indx_split)\n",
        "  indx_test = np.arange(indx_split, len(y))\n",
        "  \n",
        "  t_train = t[indx_train]\n",
        "  y_train = y[indx_train]\n",
        "  y_train = y_train.reshape(-1, 1)\n",
        "  \n",
        "  t_test = t[indx_test]\n",
        "  y_test = y[indx_test]\n",
        "  y_test = y_test.reshape(-1, 1)\n",
        "  \n",
        "  return t_train, y_train, t_test, y_test \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzadnBeOeUzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_train, y_train, t_test, y_test = train_test_split(t, y, split = 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYX9rWG7g7fq",
        "colab_type": "text"
      },
      "source": [
        "Let's plot the train/test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagy6ymJeZp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot train/test data\n",
        "## your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvnAvQZkgIxr",
        "colab_type": "text"
      },
      "source": [
        "<h3> Creating a Windowed Dataset <h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBI9A6Sji9Kj",
        "colab_type": "text"
      },
      "source": [
        "We need to organize our data into sequences of $n_{i}$ input values and $n_{o}$ target values. To do this, we slide a moving window over the dataset. We start at the first $y$ value and collect $n_{i}$ values as inputs and the the next $n_{o}$ values as targets. Then, we slide our window to the second (stride = 1) or third (stride = 2) $y$ and repeat the procedure. The windowed dataset for $n_{i}=3$ and $n_{o} = 2$ and stride = 1 is shown below.  \n",
        "\n",
        "<center> <img src=\"https://i.imgur.com/nBQUU61.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bhEPToBz3QE",
        "colab_type": "text"
      },
      "source": [
        "In order to input our data into the LSTM, the data shape must be [sequence length, batch size, # features]. The size of $X$ should be [$n_{i}$, # of times the window fits data, 1], where the last value is 1 because we only have one feature, $y$. Similarly, the size of $Y$ should be [$n_{o}$ # of times the window fits data, 1]. In the cell below, we window the data and make sure it has the right dimensions for the LSTM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj0m2H2YvS9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(y, input_window = 5, output_window = 1, stride = 1, num_features = 1):\n",
        "    '''\n",
        "    : param y: time series feature\n",
        "    : param input_window: number of y samples to give model \n",
        "    : param output_window: number of future y samples to predict  \n",
        "    : param stide: spacing between windows \n",
        "    : param num_features: number of features (i.e., 1 for us, but we could have multiple features)\n",
        "    : return: array with correct dimensions for LSTM (i.e., [# samples, time steps, # features])\n",
        "    '''\n",
        "    L = y.shape[0]\n",
        "    num_samples = (L - input_window - output_window) // stride + 1\n",
        "\n",
        "    X = np.zeros([input_window, num_samples, num_features])\n",
        "    Y = np.zeros([output_window, num_samples, num_features])    \n",
        "    \n",
        "    for ff in np.arange(num_features):\n",
        "        for ii in np.arange(num_samples):\n",
        "            start_x = stride * ii\n",
        "            end_x = start_x + input_window\n",
        "            X[:, ii, ff] = y[start_x:end_x, ff]\n",
        "\n",
        "            start_y = stride * ii + input_window\n",
        "            end_y = start_y + output_window \n",
        "            Y[:, ii, ff] = y[start_y:end_y, ff]\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCAySDDPxpyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set size of input/output windows \n",
        "iw = 80 \n",
        "ow = 20 \n",
        "s = 5\n",
        "\n",
        "# generate windowed training/test datasets\n",
        "Xtrain, Ytrain = windowed_dataset(y_train, input_window = iw, output_window = ow, stride = s)\n",
        "Xtest, Ytest = windowed_dataset(y_test, input_window = iw, output_window = ow, stride = s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WeuOW_E1jtr",
        "colab_type": "code",
        "outputId": "8cfed923-7a8f-4bdc-f8bc-69b9609d6a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# check the shape of the data \n",
        "print(f'Xtrain has shape {Xtrain.shape} and Ytrain has shape {Ytrain.shape}')\n",
        "print(f'Xtest has shape {Xtest.shape} and Ytest has shape {Ytest.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xtrain has shape (80, 301, 1) and Ytrain has shape (20, 301, 1)\n",
            "Xtest has shape (80, 61, 1) and Ytest has shape (20, 61, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2aw_bTzMGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot an example in the windowed data \n",
        "## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rcAJdmSmf-K",
        "colab_type": "text"
      },
      "source": [
        "<h2> 2. Build LSTM Encoder-Decoder in PyTorch<h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y57RlpRGmobo",
        "colab_type": "text"
      },
      "source": [
        "We will use PyTorch to build the LSTM encoder-decoder. Although we build the encoder and the decoder separately, these two modules work together when we use the model to train and predict. \n",
        "\n",
        "<center> <img src=\"https://i.imgur.com/05OKp8k.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "For the encoder, we initialize the cell and hidden states to zero. We define a `forward` function for the encoder and decoder. This represents propogation through the graph. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnyi1q3Jgsj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class lstm_encoder(nn.Module):\n",
        "    ''' Encodes time-series sequence '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
        "        '''\n",
        "        : param input_size: the number of expected features in the input x\n",
        "        : param hidden_size: the number of features in the hidden state h\n",
        "        : param num_layers: number of recurrent layers (i.e., 2 means there are\n",
        "        :                   2 stacked LSTMs)\n",
        "        '''\n",
        "        super(lstm_encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # define LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                            num_layers = num_layers)\n",
        "\n",
        "    def forward(self, x_input):\n",
        "        '''\n",
        "        : param x_input: input of shape (seq_len, # in batch, input_size)\n",
        "        '''\n",
        "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
        "        return lstm_out, self.hidden     \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        initialize hidden state\n",
        "        : param batch_size: x_input.shape[1]\n",
        "        '''\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
        "\n",
        "\n",
        "class lstm_decoder(nn.Module):\n",
        "    ''' Decodes hidden state output by encoder '''\n",
        "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
        "        super(lstm_decoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                            num_layers = num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, input_size)           \n",
        "\n",
        "    def forward(self, x_input, encoder_hidden_states):\n",
        "        '''\n",
        "        LSTMCell takes in (batch size, input size/# features)\n",
        "        : param x_input: should be 2D (batch_size, input_size)\n",
        "        '''\n",
        "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
        "        output = self.linear(lstm_out.squeeze(0))     \n",
        "        \n",
        "        return output, self.hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDEdbsznNpc",
        "colab_type": "text"
      },
      "source": [
        "<h2> 3. Train the LSTM Encoder-Decoder and make Predictions <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhZyf9QXESkf",
        "colab_type": "text"
      },
      "source": [
        "There are a few ways we can train the LSTM encoder-decoder. First, we can recurrently feed the predicted decoder outputs into the LSTM decoder until we have an output sequence of the desired length. This is the typical model structure we have considered so far.   \n",
        "\n",
        "<center> <img src=\"https://i.imgur.com/v8SYiEM.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "Instead of feeding the predicted outputs into the LSTM decoder, we could feed in the true target values. This is called teacher forcing. In training, teacher forcing acts as \"training wheels\" --- if the model makes a bad prediction, it is put back in place with the true value. \n",
        "<center> <img src=\"https://i.imgur.com/uhdJJFp.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "Another alternative is to randomly use teacher forcing (the \"training wheels\" are on some of the time). \n",
        "<center> <img src=\"https://i.imgur.com/KQoPfmZ.png\" alt=\"\" width=\"900\"> </center> \n",
        "\n",
        "In each case, we compare the predictions provided by the LSTM decoder to the true values, compute the loss, and update the weights matrices/biases in the LSTM gates (via  backpropogation) to minimize the loss. The function `train_model` carries out this training process, and allows the user to decide how much teacher forcing to use. \n",
        "\n",
        "We also define the function `make_prediction`, which allows us to make predictions once the model has been trained. The predictions are recursive. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYiHA2-HnDIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class lstm_seq2seq(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \n",
        "        super(lstm_seq2seq, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size)\n",
        "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size)\n",
        "\n",
        "\n",
        "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, teacher_forcing_ratio, learning_rate = 0.01, dynamic_tf = False):\n",
        "        '''\n",
        "        train data using teacher forcing\n",
        "        : param teacher_forcing_ratio: float [0, 1); high means more teacher forcing \n",
        "        '''\n",
        "        # initialize array of losses \n",
        "        losses = np.full(n_epochs, np.nan)\n",
        "        losses_tf = np.full(n_epochs, np.nan)\n",
        "        losses_no_tf = np.full(n_epochs, np.nan)\n",
        "\n",
        "\n",
        "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # calculate number of batch iterations\n",
        "        n_batches = int(input_tensor.shape[1] / batch_size)\n",
        "\n",
        "        with tnrange(n_epochs) as tr:\n",
        "            for it in tr:\n",
        "                \n",
        "                batch_loss = 0.\n",
        "                batch_loss_tf = 0.\n",
        "                batch_loss_no_tf = 0.\n",
        "                num_tf = 0\n",
        "                num_no_tf = 0\n",
        "\n",
        "                for b in range(n_batches):\n",
        "                    # select data \n",
        "                    input_batch = input_tensor[:, b: b + batch_size, :]\n",
        "                    target_batch = target_tensor[:, b: b + batch_size, :]\n",
        "\n",
        "                    # outputs tensor\n",
        "                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2])\n",
        "\n",
        "                    # initialize hidden state\n",
        "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
        "\n",
        "                    # zero the gradient\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # encoder outputs\n",
        "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
        "\n",
        "                    # decoder with teacher forcing\n",
        "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
        "                    decoder_hidden = encoder_hidden\n",
        "\n",
        "                    use_teacher_forcing = ( random.random() < teacher_forcing_ratio )\n",
        "\n",
        "                    if use_teacher_forcing:\n",
        "                        # teacher forcing: feed the target as the next input\n",
        "                        for t in range(target_len): \n",
        "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                            outputs[t] = decoder_output\n",
        "                            decoder_input = target_batch[t, :, :]\n",
        "\n",
        "                    else:\n",
        "                        # predict recursively\n",
        "                        for t in range(target_len): \n",
        "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                            outputs[t] = decoder_output\n",
        "                            decoder_input = decoder_output\n",
        "\n",
        "                    loss = criterion(outputs, target_batch)\n",
        "\n",
        "                    batch_loss += loss.item()\n",
        "                    \n",
        "                    if use_teacher_forcing:\n",
        "                        num_tf += batch_size\n",
        "                        batch_loss_tf += loss.item()\n",
        "                    else:\n",
        "                        num_no_tf += batch_size\n",
        "                        batch_loss_no_tf += loss.item()\n",
        "\n",
        "                    loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                batch_loss /= (n_batches * batch_size)\n",
        "                losses[it] = batch_loss\n",
        "\n",
        "                \n",
        "                if num_no_tf != 0.: \n",
        "                    batch_loss_no_tf /= num_no_tf\n",
        "                    losses_no_tf[it] = batch_loss_no_tf\n",
        "\n",
        "                    \n",
        "                if num_tf !=0:\n",
        "                    batch_loss_tf /= num_tf\n",
        "                    losses_tf[it] = batch_loss_tf\n",
        "\n",
        "                if dynamic_tf and teacher_forcing_ratio > 0:\n",
        "                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02 \n",
        "                    \n",
        "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
        "                    \n",
        "        return losses, losses_tf, losses_no_tf\n",
        "\n",
        "    def predict(self, input_tensor, target_len):\n",
        "        '''\n",
        "        : param input_tensor: (seq_len, input_size)\n",
        "        '''\n",
        "\n",
        "        # encode input_tensor\n",
        "        input_tensor = input_tensor.unsqueeze(1)     # add in batch size of 1\n",
        "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
        "\n",
        "        # initialize tensor for predictions\n",
        "        outputs = torch.zeros(target_len, input_tensor.shape[2])\n",
        "\n",
        "        # decode input_tensor\n",
        "        decoder_input = input_tensor[-1, :, :]\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        for t in range(target_len):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs[t] = decoder_output.squeeze(0)\n",
        "            decoder_input = decoder_output\n",
        "            \n",
        "        np_outputs = outputs.detach().numpy()\n",
        "        \n",
        "        return np_outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmx6VYBoQkBB",
        "colab_type": "text"
      },
      "source": [
        "<h2>4. Evaluate the LSTM Encoder-Decoder on the Train/Test Sets<h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQzsHZ1QsnG",
        "colab_type": "text"
      },
      "source": [
        "Let's train a model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwF8OO-V551D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the data needs to be in a torch format, not np.array\n",
        "X_train = torch.from_numpy(Xtrain).type(torch.Tensor)\n",
        "Y_train = torch.from_numpy(Ytrain).type(torch.Tensor)\n",
        "\n",
        "X_test = torch.from_numpy(Xtest).type(torch.Tensor)\n",
        "Y_test = torch.from_numpy(Ytest).type(torch.Tensor)\n",
        "input_size = X_train.shape[2]\n",
        "\n",
        "# specify model parameters and train \n",
        "model = lstm_seq2seq(input_size = input_size, hidden_size = 15)\n",
        "loss, loss_tf, loss_no_tf = model.train_model(X_train, Y_train, n_epochs = 10, target_len = ow, batch_size = 5, teacher_forcing_ratio = 0.1, learning_rate = 0.01, dynamic_tf = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk22h2YgQ7U8",
        "colab_type": "text"
      },
      "source": [
        "Let's see how the model performs on the train/test data by plotting a few examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OsBXuhh5_L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_test_results(lstm_model, num_rows = 4): \n",
        "  num_cols = 2\n",
        "  num_plots = num_rows * num_cols\n",
        "\n",
        "  fig, ax = plt.subplots(num_rows, num_cols, figsize = (8, 12))\n",
        "\n",
        "  for ii in range(num_rows):\n",
        "      # train set\n",
        "      X_train_plt = X_train[:, ii, :]\n",
        "      Y_train_pred = lstm_model.predict(X_train_plt, target_len = ow)\n",
        "      ax[ii, 0].plot(np.arange(0, iw), Xtrain[:, ii, 0], 'k', label = 'Input')\n",
        "      ax[ii, 0].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtrain[-1, ii, 0]], Ytrain[:, ii, 0]]), 'b', label = 'Target')\n",
        "      ax[ii, 0].plot(np.arange(iw - 1, iw + ow),  np.concatenate([[Xtrain[-1, ii, 0]], Y_train_pred[:, 0]]), 'r', label = 'Prediction')\n",
        "      ax[ii, 0].set_xlabel('t')\n",
        "      ax[ii, 0].set_ylabel('y')\n",
        "\n",
        "\n",
        "      # test set\n",
        "      X_test_plt = X_test[:, ii, :]\n",
        "      Y_test_pred = lstm_model.predict(X_test_plt, target_len = ow)\n",
        "      ax[ii, 1].plot(np.arange(0, iw), Xtest[:, ii, 0], 'k', label = 'Input')\n",
        "      ax[ii, 1].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtest[-1, ii, 0]], Ytest[:, ii, 0]]), 'b', label = 'Target')\n",
        "      ax[ii, 1].plot(np.arange(iw - 1, iw + ow), np.concatenate([[Xtest[-1, ii, 0]], Y_test_pred[:, 0]]), 'r', label = 'Prediction')\n",
        "      ax[ii, 1].set_xlabel('t')\n",
        "      ax[ii, 1].set_ylabel('y')\n",
        "\n",
        "      if ii == 0:\n",
        "        ax[ii, 0].set_title('Train')\n",
        "        \n",
        "        ax[ii, 1].legend(bbox_to_anchor=(1, 1))\n",
        "        ax[ii, 1].set_title('Test')\n",
        "\n",
        "        \n",
        "  return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNnFnZk5Rlez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_test_results(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBHOQffwVh-z",
        "colab_type": "text"
      },
      "source": [
        "<h2> Your Turn <h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CENm8thcVliC",
        "colab_type": "text"
      },
      "source": [
        "Now you can play around with the LSTM encoder-decoder. You can try changing the\n",
        "\n",
        "\n",
        "*    function used to generate the dataset \n",
        "*    input/output window size\n",
        "*    number of epochs and the batch size\n",
        "*    amount of teacher forcing (0 = no teacher forcing, 1 = full teacher forcing)\n",
        "*    learning rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zb_y_byTgghv",
        "colab": {}
      },
      "source": [
        "# your model \n",
        "hidden_size = 15 \n",
        "n_epochs = 1 \n",
        "batch_size = 5\n",
        "tf_ratio = 0.5\n",
        "learning_rate = 0.01 \n",
        "\n",
        "model_new = lstm_seq2seq(input_size = input_size, hidden_size = hidden_size)\n",
        "loss, loss_tf, loss_no_tf = model_new.train_model(X_train, Y_train, n_epochs = n_epochs, target_len = ow, batch_size = batch_size, teacher_forcing_ratio = tf_ratio, learning_rate = learning_rate, dynamic_tf = False)\n",
        "plot_train_test_results(model_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAZl835RiCl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}